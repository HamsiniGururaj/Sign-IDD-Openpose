{"cells":[{"cell_type":"markdown","source":["# **SIGN IDD: Iconicity Disentangled Diffusion for Sign Language Production**"],"metadata":{"id":"7ugMty9Zhp7T"},"id":"7ugMty9Zhp7T"},{"cell_type":"markdown","source":["# Initial imports and configuration"],"metadata":{"id":"AVEiF2i1hhym"},"id":"AVEiF2i1hhym"},{"cell_type":"code","execution_count":1,"id":"M6mNk3oVZqVB","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42864,"status":"ok","timestamp":1752555197388,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"},"user_tz":-330},"id":"M6mNk3oVZqVB","outputId":"c16f896a-bc18-41b3-8ffe-2e8ab45cb357"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":2,"id":"3db21ded-c15d-4c00-9c51-0fe80dd95263","metadata":{"executionInfo":{"elapsed":4171,"status":"ok","timestamp":1752555204011,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"},"user_tz":-330},"id":"3db21ded-c15d-4c00-9c51-0fe80dd95263"},"outputs":[],"source":["#import packages\n","import os\n","import torch\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import time\n","import shutil #for copying configs (directory operations)\n","import queue #for managing checkpoints"]},{"cell_type":"code","execution_count":3,"id":"389de229-ed02-4cd7-9027-cb26315e105e","metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1752555205661,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"},"user_tz":-330},"id":"389de229-ed02-4cd7-9027-cb26315e105e"},"outputs":[],"source":["device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":4,"id":"bc8966a9-41b6-4623-bcc6-bad94c7de5bc","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1752555207577,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"},"user_tz":-330},"id":"bc8966a9-41b6-4623-bcc6-bad94c7de5bc"},"outputs":[],"source":["config_path = '/content/drive/MyDrive/Sign IDD Openpose/Sign-IDD/Configs/Sign-IDD.yaml'\n","ckpt = '/content/drive/MyDrive/Sign IDD Openpose/Sign-IDD/Checkpoints/final_model.pth'"]},{"cell_type":"code","execution_count":5,"id":"CTR3p7UJaBFP","metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1752555209081,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"},"user_tz":-330},"id":"CTR3p7UJaBFP"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/Sign IDD Openpose/Sign-IDD')"]},{"cell_type":"code","execution_count":null,"id":"xS5IdcU9bxia","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1841,"status":"ok","timestamp":1752465557976,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"},"user_tz":-330},"id":"xS5IdcU9bxia","outputId":"803322e0-7ae3-4a8a-ccbe-f6a9672636c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Checkpoints', 'loss.py', 'dtw.py', 'transformer_layers.py', 'Configs', '.ipynb_checkpoints', 'model_architecture.png', 'ACD.py', 'Data', 'Models', 'initialization.py', 'model_architecture', 'data.py', 'eval_helper.py', 'plot_videos.py', 'helpers.py', 'model.py', 'encoder.py', 'ID.py', 'prediction.py', 'torchtext_compat.py', 'embeddings.py', 'vocabulary.py', 'builders.py', '__pycache__', 'ACD_Denoiser (1).py', 'batch (1).py', 'constants (1).py', 'constants.py', 'ACD_Denoiser.py', 'batch.py', 'Main.ipynb']\n","False\n"]}],"source":["print(os.listdir('/content/drive/MyDrive/Sign IDD Openpose/Sign-IDD'))\n","print(\"constants\" in sys.modules)\n","import constants"]},{"cell_type":"code","execution_count":6,"id":"tbeCYfP0RBCP","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39018,"status":"ok","timestamp":1752555251455,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"},"user_tz":-330},"id":"tbeCYfP0RBCP","outputId":"36c5a7b8-d903-4b92-c682-3db140fff1b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorboardX\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.5)\n","Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6.4\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=6c7a4634a60cfb4afbb8449ddc79f4bc6911558a62bc27af7d8a6db14d155fb7\n","  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n","Collecting jiwer\n","  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n","Collecting rapidfuzz>=3.9.7 (from jiwer)\n","  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n","Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n","Successfully installed jiwer-4.0.0 rapidfuzz-3.13.0\n"]}],"source":["%pip install tensorboardX\n","%pip install rouge_score\n","%pip install jiwer\n"]},{"cell_type":"code","execution_count":7,"id":"fbd67c90-5d68-42f5-9649-8b00f34fe706","metadata":{"executionInfo":{"elapsed":12071,"status":"ok","timestamp":1752555268326,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"},"user_tz":-330},"id":"fbd67c90-5d68-42f5-9649-8b00f34fe706"},"outputs":[],"source":["#import custom modules\n","from torch import Tensor\n","import data\n","from data import load_data\n","import torchtext_compat\n","from torchtext_compat import make_data_iterator, Batch, TextField, RawField\n","#import batch\n","#from batch import Batch\n","from helpers import load_config, set_seed, load_checkpoint, log_cfg, make_model_dir, make_logger, ConfigurationError, get_latest_checkpoint, symlink_update\n","from model import build_model, Model\n","from tensorboardX import SummaryWriter\n","from constants import TARGET_PAD\n","from loss import Loss\n","from builders import build_gradient_clipper, build_optimizer, build_scheduler\n","from torch.utils.data import Dataset\n","from plot_videos import plot_video, alter_DTW_timing\n","from prediction import validate_on_data"]},{"cell_type":"markdown","id":"6caf255c-18d5-43dd-bb54-a097d255f172","metadata":{"id":"6caf255c-18d5-43dd-bb54-a097d255f172"},"source":["# Train Manager Class"]},{"cell_type":"code","execution_count":8,"id":"5e713693-5326-4072-8f61-5fd70a69e7e2","metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1752555272377,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"},"user_tz":-330},"id":"5e713693-5326-4072-8f61-5fd70a69e7e2"},"outputs":[],"source":["class TrainManager:\n","    def __init__(self, model: Model, config: dict, test= False):\n","        train_config= config[\"training\"]  #load the configs specific to training from the yaml file\n","        model_dir= train_config[\"model_dir\"]  #get the dir where model checkpoints and logs are stored\n","        model_continue = train_config.get(\"continue\", True)  #continue from the latest checkpoint if true\n","\n","        #if directory does not exist, cannot continue from anything, if in test mode, force continuation\n","        if not os.path.isdir(model_dir):\n","            model_continue = False\n","        if test:\n","            model_continue = True\n","\n","        self.use_cuda = False\n","\n","        #create model dir if it doesnt exist\n","        self.model_dir = make_model_dir(train_config[\"model_dir\"],\n","                                overwrite=train_config.get(\"overwrite\", False),\n","                                model_continue=model_continue)\n","\n","        #logger setup\n","        self.logger = make_logger(model_dir=self.model_dir)\n","        self.logging_freq = train_config.get(\"logging_freq\", 100)\n","        self.tb_writer = SummaryWriter(log_dir=self.model_dir+\"/tensorboard/\")\n","        self.valid_report_file = \"{}/validations.txt\".format(self.model_dir)  #file where validation scores will be written\n","\n","        #model\n","        self.model = model  #store model\n","        self.pad_index = self.model.pad_index\n","        self.bos_index = self.model.bos_index\n","        self._log_parameters_list()  #log model parameters for reference\n","\n","        #initialise loss object with loss function (L1 loss) and padding info\n","        self.target_pad = TARGET_PAD\n","        self.loss = Loss(cfg=config, target_pad=self.target_pad)\n","\n","        #do gradient normalisation per batch\n","        self.normalization = \"batch\"\n","\n","        #optimisation settings\n","        self.learning_rate_min = train_config.get(\"learning_rate_min\", 1.0e-8)\n","        self.clip_grad_fun = build_gradient_clipper(config=train_config)\n","        self.optimizer = build_optimizer(config=train_config, parameters=model.parameters())  #adam\n","\n","        #validation and early stopping\n","        self.validation_freq = train_config.get(\"validation_freq\", 1000)\n","        self.ckpt_best_queue = queue.Queue(maxsize=train_config.get(\"keep_last_ckpts\", 1))  #maintain a queue for best checkpoints\n","        self.ckpt_queue = queue.Queue(maxsize=1)\n","\n","        #setup evaluation metrics and early stopping criteria\n","        self.eval_metric = train_config.get(\"eval_metric\", \"dtw\").lower()\n","        if self.eval_metric not in ['bleu', 'chrf', \"dtw\"]:\n","            raise ConfigurationError(\"Invalid setting for 'eval_metric', \"\n","                                     \"valid options: 'bleu', 'chrf', 'DTW'\")\n","\n","        self.early_stopping_metric = train_config.get(\"early_stopping_metric\", \"eval_metric\") #choose which metric to use for early stopping\n","\n","        if self.early_stopping_metric in [\"loss\", \"dtw\"]: #if using loss or dtw, we want lower values, so minimize=true\n","            self.minimize_metric = True\n","        else:\n","            raise ConfigurationError(\"Invalid setting for 'early_stopping_metric', \"\n","                                    \"valid options: 'loss', 'dtw',.\")\n","\n","        #learning rate scheduling (plateau)\n","        self.scheduler, self.scheduler_step_at = build_scheduler(\n","            config=train_config,\n","            scheduler_mode=\"min\" if self.minimize_metric else \"max\",\n","            optimizer=self.optimizer,\n","            hidden_size=config[\"model\"][\"encoder\"][\"hidden_size\"])\n","\n","        #data and batch handling\n","        self.level = \"word\"  #tokenization level\n","        self.shuffle = train_config.get(\"shuffle\", True)  #shuffling enabled\n","        self.epochs = train_config[\"epochs\"]\n","        self.batch_size = train_config[\"batch_size\"]  #for both training and evaluation\n","        self.batch_type = \"sentence\"\n","        self.eval_batch_size = train_config.get(\"eval_batch_size\",self.batch_size)\n","        self.eval_batch_type = train_config.get(\"eval_batch_type\",self.batch_type)\n","        self.batch_multiplier = train_config.get(\"batch_multiplier\", 1)  #accumulate gradient over 1 batch only (no gradient accumulation)\n","\n","        #decoding setup\n","        self.max_output_length = train_config.get(\"max_output_length\", None)  #max length for decoder output(used in sequence generation)\n","\n","        #device setup\n","        self.device = torch.device(\"cuda\" if train_config[\"use_cuda\"] and torch.cuda.is_available() else \"cpu\")\n","        self.model.to(self.device)\n","        self.loss.to(self.device)\n","\n","        # initialize training statistics\n","        self.steps = 0\n","        # stop training if this flag is True by reaching learning rate minimum\n","        self.stop = False\n","        self.total_tokens = 0\n","        self.best_ckpt_iteration = 0\n","        # initial values for best scores\n","        self.best_ckpt_score = np.inf if self.minimize_metric else -np.inf\n","        # comparison function for scores\n","        self.is_best = lambda score: score < self.best_ckpt_score \\\n","            if self.minimize_metric else score > self.best_ckpt_score\n","\n","        ## Checkpoint restart\n","        # If continuing\n","        #if model_continue:\n","            # Get the latest checkpoint\n","        #    ckpt = get_latest_checkpoint(model_dir)\n","        #    if ckpt is None:\n","        #        self.logger.info(\"Can't find checkpoint in directory %s\", ckpt)\n","        #    else:\n","        #        self.logger.info(\"Continuing model from %s\", ckpt)\n","        #        self.init_from_checkpoint(ckpt)\n","\n","        # Skip frames\n","        self.skip_frames = config[\"data\"].get(\"skip_frames\", 1)\n","\n","\n","\n","\n","    #function to save a checkpoint\n","    def _save_checkpoint(self, type=\"every\") -> None:\n","        # Define model path\n","        model_path = \"{}/{}_{}.ckpt\".format(self.model_dir, self.steps, type)\n","        # Define State\n","        state = {\n","            \"steps\": self.steps,\n","            \"total_tokens\": self.total_tokens,\n","            \"best_ckpt_score\": self.best_ckpt_score,\n","            \"best_ckpt_iteration\": self.best_ckpt_iteration,\n","            \"model_state\": self.model.state_dict(),\n","            \"optimizer_state\": self.optimizer.state_dict(),\n","            \"scheduler_state\": self.scheduler.state_dict() if \\\n","            self.scheduler is not None else None,\n","        }\n","        #save\n","        torch.save(state, model_path)\n","\n","        #if this is the best checkpoint, add it to the best checkpoint queue\n","        if type == \"best\":\n","            if self.ckpt_best_queue.full():\n","                to_delete = self.ckpt_best_queue.get()  # delete oldest ckpt\n","                try:\n","                    os.remove(to_delete)\n","                except FileNotFoundError:\n","                    self.logger.warning(\"Wanted to delete old checkpoint %s but \"\n","                                        \"file does not exist.\", to_delete)\n","\n","            self.ckpt_best_queue.put(model_path)\n","\n","            best_path = \"{}/best.ckpt\".format(self.model_dir)\n","            try:\n","                # create/modify symbolic link for best checkpoint\n","                symlink_update(\"{}_best.ckpt\".format(self.steps), best_path)\n","            except OSError:\n","                # overwrite best.ckpt\n","                torch.save(state, best_path)\n","\n","        #if this is just the checkpoint at every validation\n","        elif type == \"every\":\n","            if self.ckpt_queue.full():\n","                to_delete = self.ckpt_queue.get()  # delete oldest ckpt\n","                try:\n","                    os.remove(to_delete)\n","                except FileNotFoundError:\n","                    self.logger.warning(\"Wanted to delete old checkpoint %s but \"\n","                                        \"file does not exist.\", to_delete)\n","\n","            self.ckpt_queue.put(model_path)\n","\n","            every_path = \"{}/every.ckpt\".format(self.model_dir)\n","            try:\n","                # create/modify symbolic link for best checkpoint\n","                symlink_update(\"{}_best.ckpt\".format(self.steps), every_path)\n","            except OSError:\n","                # overwrite every.ckpt\n","                torch.save(state, every_path)\n","\n","\n","\n","    #function to initialise from a checkpoint\n","    def init_from_checkpoint(self, path: str) -> None:\n","        # Find last checkpoint\n","        model_checkpoint = load_checkpoint(path=path, use_cuda=self.use_cuda)\n","\n","        # restore model and optimizer parameters\n","        self.model.load_state_dict(model_checkpoint[\"model_state\"])\n","        self.optimizer.load_state_dict(model_checkpoint[\"optimizer_state\"])\n","\n","        if model_checkpoint[\"scheduler_state\"] is not None and \\\n","                self.scheduler is not None:\n","            # Load the scheduler state\n","            self.scheduler.load_state_dict(model_checkpoint[\"scheduler_state\"])\n","\n","        # restore counts\n","        self.steps = model_checkpoint[\"steps\"]\n","        self.total_tokens = model_checkpoint[\"total_tokens\"]\n","        self.best_ckpt_score = model_checkpoint[\"best_ckpt_score\"]\n","        self.best_ckpt_iteration = model_checkpoint[\"best_ckpt_iteration\"]\n","\n","        self.model.to(self.device)\n","\n","\n","\n","    #train and validate function\n","    def train_and_validate(self, train_data: Dataset, valid_data: Dataset) -> None:\n","        #make training iterator\n","\n","        train_iter= torchtext_compat.make_data_iterator(train_data,      #used to iterate through batches of processed (gloss,skels,file) triplets\n","                                       train_data.fields,\n","                                       batch_size= self.batch_size,\n","                                       shuffle= self.shuffle,\n","                                       trg_pad_val=TARGET_PAD)\n","        #print(\">>> Number of examples in train_data:\", len(train_data))\n","        #print(\">>> Fields keys:\", train_data.fields.keys())\n","        #print(\">>> Example 0 trg:\", train_data[0].trg if len(train_data) > 0 else \"Empty\")\n","\n","        #print(\"DEBUG vocab at iterator creation:\", train_data.fields[\"src\"].vocab)\n","\n","        val_step=0\n","\n","        #training loop\n","        for epoch_no in range(0,self.epochs):\n","            #self.logger.info(\"EPOCH %d\", epoch_no + 1)\n","\n","            if self.scheduler is not None and self.scheduler_step_at == \"epoch\":  #if scheduler is set to step on epoch, step the lr scheduler at the start of each epoch\n","                self.scheduler.step(epoch=epoch_no)\n","\n","            self.model.train()\n","\n","            # Reset statistics for each epoch.\n","            start = time.time()\n","            total_valid_duration = 0\n","            start_tokens = self.total_tokens\n","            count = self.batch_multiplier - 1\n","            epoch_loss = 0\n","\n","            #loop through each batch\n","            for batch in iter(train_iter):\n","                #print(\">>> First batch keys:\", batch.keys())\n","                # reactivate training\n","                self.model.train()\n","\n","                # create a Batch object from custom batch class\n","                batch = Batch(batch_dict=batch,\n","                              pad_index=self.pad_index,\n","                              target_pad=self.target_pad)\n","\n","                update= count==0  #if update is true(count-tracks batch_multiplier value), gradients will be stepped this batch\n","\n","                # Train the model on a batch\n","                batch_loss = self._train_batch(batch, update=update)\n","                self.tb_writer.add_scalar(\"train/train_batch_loss\", batch_loss, self.steps)\n","\n","                count = self.batch_multiplier if update else count\n","                count -= 1\n","                epoch_loss += batch_loss.detach().cpu().numpy()\n","\n","                if self.scheduler is not None and self.scheduler_step_at == \"step\" and update:\n","                    self.scheduler.step()\n","\n","                # log learning progress\n","                if self.steps % self.logging_freq == 0 and update:\n","                    elapsed = time.time() - start - total_valid_duration\n","                    elapsed_tokens = self.total_tokens - start_tokens\n","                    self.logger.info(\n","                        \"Epoch %d | Step: %d | Batch Loss: %.6f | \"\n","                        \"Tokens per Sec: %.0f | Lr: %.6f\",\n","                        epoch_no + 1, self.steps, batch_loss,\n","                        elapsed_tokens / elapsed,\n","                        self.optimizer.param_groups[0][\"lr\"])\n","                    start = time.time()\n","                    total_valid_duration = 0\n","                    start_tokens = self.total_tokens\n","\n","                #validation block- runs after every validation_freq steps\n","                if self.steps % self.validation_freq == 0 and update:\n","                    valid_start_time = time.time()\n","\n","                    valid_score, valid_loss, valid_references, valid_hypotheses, \\\n","                        valid_inputs, all_dtw_scores, valid_file_paths, valid_bt_f1, valid_mpjpe,_,_,_,_,_,_= \\\n","                        validate_on_data(\n","                            batch_size=self.eval_batch_size,\n","                            data=valid_data,\n","                            eval_metric=self.eval_metric,\n","                            model=self.model,\n","                            max_output_length=self.max_output_length,\n","                            loss_function=self.loss,\n","                            batch_type=self.eval_batch_type,\n","                            type=\"val\",\n","                        )\n","\n","                    val_step+=1\n","\n","                     # Tensorboard writer\n","                    self.tb_writer.add_scalar(\"Valid/loss\", valid_loss, self.steps)\n","                    self.tb_writer.add_scalar(\"Valid/dtw\", valid_score, self.steps)\n","                    self.tb_writer.add_scalar(\"Valid/bt_f1\", valid_bt_f1, self.steps)\n","                    self.tb_writer.add_scalar(\"Valid/mpjpe\", valid_mpjpe, self.steps)\n","\n","\n","                    if self.early_stopping_metric == \"loss\":\n","                        ckpt_score = valid_loss\n","                    elif self.early_stopping_metric == \"dtw\":\n","                        ckpt_score = valid_score\n","                    else:\n","                        ckpt_score = valid_score\n","\n","                    #if this validation is better than all previous ones, save as best checkpoint\n","                    new_best = False\n","                    self.best = False\n","                    if self.is_best(ckpt_score):\n","                        self.best = True\n","                        self.best_ckpt_score = ckpt_score\n","                        self.best_ckpt_iteration = self.steps\n","                        self.logger.info(\n","                            'Hooray! New best validation result [%s]!',\n","                            self.early_stopping_metric)\n","                        if self.ckpt_queue.maxsize > 0:\n","                            self.logger.info(\"Saving new checkpoint.\")\n","                            new_best = True\n","                            self._save_checkpoint(type=\"best\")\n","\n","                        # generate video samples\n","                        display = list(range(0, len(valid_hypotheses), int(np.ceil(len(valid_hypotheses) / 13.15))))\n","                        self.produce_validation_video(\n","                            output_joints=valid_hypotheses,\n","                            inputs=valid_inputs,\n","                            references=valid_references,\n","                            model_dir=self.model_dir,\n","                            steps=self.steps,\n","                            display=display,\n","                            type=\"val_inf\",\n","                            file_paths=valid_file_paths,\n","                        )\n","\n","\n","                    #save checkpoint after every validation step even if it is not the best\n","                    self._save_checkpoint(type=\"every\")\n","\n","                    if self.scheduler is not None and self.scheduler_step_at == \"validation\":\n","                        self.scheduler.step(ckpt_score)\n","\n","                    # append to validation report\n","                    self._add_report(\n","                        valid_score=valid_score, valid_loss=valid_loss,\n","                        eval_metric=self.eval_metric,\n","                        new_best=new_best, report_type=\"val\",)\n","\n","                    valid_duration = time.time() - valid_start_time\n","                    total_valid_duration += valid_duration\n","                    self.logger.info('Validation result at epoch %d\\n'\n","                                    '\\t\\t\\tStep: %d | DTW: %.4f | BT-F1: %.4f | MPJPE: %.4f | Loss: %.4f | Duration: %.2fs',\n","                                    epoch_no + 1, self.steps, valid_score, valid_bt_f1, valid_mpjpe, valid_loss, valid_duration\n","                                    )\n","\n","\n","                if self.stop:\n","                    break\n","\n","            if self.stop:\n","                self.logger.info(\n","                    'Training ended since minimum lr %f was reached.',\n","                     self.learning_rate_min)\n","                break\n","\n","            if epoch_no % 20 == 0:\n","                self.logger.info('Epoch %d: Total training loss %.5f', epoch_no,\n","                                 epoch_loss)\n","\n","        else:\n","            self.logger.info('Training ended after %d epochs.', epoch_no+1)\n","\n","\n","        self.logger.info('Best validation result at step %d: %.2f %s.',\n","                         self.best_ckpt_iteration, self.best_ckpt_score,\n","                         self.early_stopping_metric)\n","\n","        # Save final model weights\n","        final_ckpt_path = os.path.join(\"./Checkpoints\", \"final_model.pth\")\n","        torch.save(self.model.state_dict(), final_ckpt_path)\n","        self.logger.info(\"Final model saved to %s\", final_ckpt_path)\n","\n","        self.tb_writer.close()  # close Tensorboard writer\n","\n","\n","\n","    #function to generate validation videos (side by side comparision of output and ground truth)\n","    def produce_validation_video(self, output_joints, inputs, references, display,\n","                                 model_dir, type, steps=\"\", file_paths=None, dtw_file= None):\n","\n","        # If not at test\n","        if type != \"test\":\n","            dir_name = model_dir + \"/videos/Step_{}/\".format(steps)\n","            if not os.path.exists(model_dir + \"/videos/\"):\n","                os.mkdir(model_dir + \"/videos/\")\n","\n","        # If at test time\n","        elif type == \"test\":\n","            dir_name = model_dir + \"/test_videos/\"\n","\n","        # Create model video folder if not exist\n","        if not os.path.exists(dir_name):\n","            os.mkdir(dir_name)\n","\n","        #generate one video per sample index in display\n","        for i in display:\n","            #pull the relevant data\n","            seq = output_joints[i]\n","            ref_seq = references[i]\n","            input = inputs[i]\n","\n","            #create gloss label\n","            gloss_label = input[0]\n","            if input[1] != \"</s>\":  #skip eos tokens\n","                gloss_label += \"_\" + input[1]\n","            if input[2] != \"</s>\":\n","                gloss_label += \"_\" + input[2]\n","\n","            # Alter the dtw timing of the produced sequence using the reference sequence, and collect the DTW score\n","            timing_hyp_seq, ref_seq_count, dtw_score = alter_DTW_timing(seq, ref_seq)\n","\n","            #name the file using gloss and score\n","            video_ext = \"{}_{}.mp4\".format(gloss_label, \"{0:.2f}\".format(float(dtw_score)).replace(\".\", \"_\"))\n","\n","            #get sequence id to track source files\n","            if file_paths is not None:\n","                sequence_ID = file_paths[i]\n","            else:\n","                sequence_ID = None\n","\n","            #print and log dtw score\n","            print(sequence_ID + '    dtw: ' + '{0:.2f}'.format(float(dtw_score)))\n","            if dtw_file != None:\n","                dtw_file.writelines(sequence_ID + ' ' + '{0:.2f}'.format(float(dtw_score)) + '\\n')\n","\n","            #plot the video\n","            os.makedirs(dir_name, exist_ok=True)\n","            plot_video(joints=timing_hyp_seq,\n","                       file_path=dir_name,\n","                       video_name=None,\n","                       references=ref_seq_count,\n","                       skip_frames=self.skip_frames,\n","                       sequence_ID=sequence_ID)\n","\n","\n","\n","    #function to save the skeletons\n","    def save_skels(self, output_joints, display, model_dir, type, file_paths=None):\n","        #create pickle output file\n","        picklefile = open(model_dir + \"/phoenix14t.skels.%s\" % type, \"wb\")\n","\n","        #load metadata csv\n","        csvIn = pd.read_csv(model_dir + \"/csv/%s_phoenix2014t.csv\" % type, sep='|',encoding='utf-8')\n","        pickle_list = []\n","\n","        #loop through selected samples\n","        for i in display:\n","            name = file_paths[i]\n","            #extract metadata\n","            video = name[len(os.path.dirname(name))+1:]\n","            signer = csvIn[csvIn['id']==video]['signer'].item()\n","            gloss = csvIn[csvIn['id']==video]['annotation'].item()\n","            text = csvIn[csvIn['id']==video]['translation'].item()\n","            #extract predicted joint sequences\n","            seq = output_joints[i].cpu()[:,:-1]\n","            sign = torch.tensor(seq, dtype = torch.float32)\n","\n","            #build and store dictionary\n","            dict_num = {'name': name, 'signer': signer, 'gloss': gloss, 'text': text, 'sign': sign}\n","\n","            pickle_list.append(dict_num)\n","\n","        pickle.dump(pickle_list, picklefile)\n","        print(\"The skeletons of %s date have been save.\" % type)\n","\n","    def save_predicted_skels_txt(self, output_joints, display, output_path):\n","        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n","           for i in display:\n","               seq = output_joints[i].cpu()\n","                # remove last dim if needed (e.g., 4D: x, y, z, conf → 3D)\n","               if seq.shape[-1] % 3 == 0 and seq.shape[-1] > 3:\n","                    seq = seq[:, :-1]\n","               flat = seq.contiguous().view(-1).tolist()\n","               f.write(\" \".join(f\"{x:.6f}\" for x in flat) + \"\\n\")\n","        print(f\"[✔] Saved predicted skeletons to {output_path}\")\n","\n","\n","\n","\n","\n","    #function to train 1 batch\n","    def _train_batch(self, batch: Batch, update: bool = True) -> Tensor:\n","        #compute loss\n","        batch_loss = self.model.get_loss_for_batch(is_train=True,\n","                                                          batch=batch,\n","                                                          loss_function=self.loss)\n","\n","        #determine what to divide the loss by (for normalisation)\n","        if self.normalization == \"batch\":\n","            normalizer = batch.nseqs\n","        elif self.normalization == \"tokens\":\n","            normalizer = batch.ntokens\n","        else:\n","            raise NotImplementedError(\"Only normalize by 'batch' or 'tokens'\")\n","\n","\n","        norm_batch_loss= batch_loss/normalizer\n","        #divide further so that gradient is accumulated over batch_multiplier\n","        norm_batch_multiply= norm_batch_loss/self.batch_multiplier\n","\n","        #backpropagate\n","        norm_batch_multiply.backward()\n","\n","\n","        #if gradient clipping function is defined, it is applied to prevent exploding gradients\n","        if self.clip_grad_fun is not None:\n","            self.clip_grad_fun(params=self.model.parameters())\n","\n","        #update parameters\n","        if update:\n","            self.optimizer.step()\n","            self.optimizer.zero_grad()\n","\n","            # increment step counter\n","            self.steps += 1\n","\n","        #update total count token\n","        self.total_tokens += batch.ntokens\n","\n","        return norm_batch_loss\n","\n","    #function to log validation performance after an evaluation step\n","    def _add_report(self,valid_score: float, valid_loss: float, eval_metric: str, new_best: bool= False, report_type:str=\"val\")->None:\n","        current_lr=-1;\n","        #extract the learning rate from the current parameter group\n","        for param_group in self.optimizer.param_groups:\n","            current_lr = param_group['lr']\n","\n","        #check for early stopping\n","        if current_lr < self.learning_rate_min:\n","            self.stop = True\n","\n","        #log to the validation report file\n","        if report_type == \"val\":\n","            with open(self.valid_report_file, 'a') as opened_file:\n","                opened_file.write(\n","                    \"Steps: {} Loss: {:.5f}| DTW: {:.3f}|\"\n","                    \" LR: {:.6f} {}\\n\".format(\n","                        self.steps, valid_loss, valid_score,\n","                        current_lr, \"*\" if new_best else \"\"))\n","\n","\n","    #function to write all model parameters to the log\n","    def _log_parameters_list(self)->None:\n","        #filter only those parameters whose values are updated during training (trainable parameters)\n","        model_parameters = filter(lambda p: p.requires_grad,\n","                                  self.model.parameters())\n","\n","        n_params = sum([np.prod(p.size()) for p in model_parameters])\n","        self.logger.info(\"Total params: %d\", n_params)\n","\n","        #collect the names of all the trainable parameters\n","        trainable_params = [n for (n, p) in self.model.named_parameters()\n","                            if p.requires_grad]\n","\n","        self.logger.info(\"Trainable parameters: %s\", sorted(trainable_params))\n","        assert trainable_params  #throws assertion error if no trainable parameters found\n"]},{"cell_type":"markdown","id":"6d0371d1","metadata":{"id":"6d0371d1"},"source":["# Train function"]},{"cell_type":"code","execution_count":9,"id":"0cdf151b-87af-4ddd-a0f6-ead95c7d3569","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1752555285487,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"},"user_tz":-330},"id":"0cdf151b-87af-4ddd-a0f6-ead95c7d3569"},"outputs":[],"source":["def train (cfg_file:str, ckpt):\n","    cfg= load_config(cfg_file)  #cfg is a dictionary of all the configurations from the yaml file\n","    set_seed(seed=cfg[\"training\"].get(\"random_seed\", 42)) #for reproducibility\n","\n","    #prepare the dataset and load it using the functions in data.py\n","    train_data, dev_data, test_data, src_vocab, trg_vocab = load_data(cfg=cfg)\n","\n","    #print(\"train_data sample:\")\n","    #print(\"src:\", train_data[0].src)\n","    #print(\"trg:\", train_data[0].trg)\n","    #print(\"file_paths:\",train_data[0].file_paths)\n","\n","    #instantiate the model\n","    model = build_model(cfg=cfg, src_vocab=src_vocab, trg_vocab=trg_vocab)\n","\n","    #if there are checkpoints, load parameters from there instead of initialising\n","    if ckpt is not None and os.path.getsize(ckpt)>0:\n","        device= torch.device('cuda' if torch.cuda.is_available() and cfg[\"training\"].get(\"use_cuda\", True) else 'cpu')\n","        model_checkpoint= torch.load(ckpt, map_location= device)\n","        #model.load_state_dict(model_checkpoint[\"model_state\"])\n","        model.load_state_dict(torch.load(ckpt, map_location=device))\n","\n","    #initialise the train manager class\n","    trainer = TrainManager(config=cfg, model=model, test=False)\n","\n","    #store a copy of the training config before training loop\n","    shutil.copy2(cfg_file, trainer.model_dir+\"/Sign-IDD.yaml\")\n","    log_cfg(cfg, trainer.logger)\n","\n","    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)"]},{"cell_type":"code","execution_count":null,"id":"6dea49ab","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"113l4WiA5Pq8r7Mj_w0ZsziWHF7k5u21i"},"id":"6dea49ab","outputId":"acdf2fa2-ff7b-4f46-a087-b0d7e5bc3130","scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["train(cfg_file= config_path, ckpt=ckpt)"]},{"cell_type":"markdown","id":"b0931489","metadata":{"id":"b0931489"},"source":["# Test function"]},{"cell_type":"code","source":["def test(cfg_file, ckpt: str):\n","    cfg = load_config(cfg_file)\n","    model_dir = cfg[\"training\"][\"model_dir\"]\n","\n","    batch_size = cfg[\"training\"].get(\"eval_batch_size\", cfg[\"training\"][\"batch_size\"])\n","    batch_type = cfg[\"training\"].get(\"eval_batch_type\", cfg[\"training\"].get(\"batch_type\", \"sentence\"))\n","    use_cuda = cfg[\"training\"].get(\"use_cuda\", True)\n","    eval_metric = cfg[\"training\"][\"eval_metric\"]\n","    max_output_length = cfg[\"training\"].get(\"max_output_length\", None)\n","\n","    # create datasets for model input\n","    _, dev_data, test_data, src_vocab, trg_vocab = load_data(cfg=cfg)\n","    data_to_predict = {\"dev\": dev_data, \"test\": test_data}\n","\n","    # load model architecture\n","    trained_model = build_model(cfg, src_vocab, trg_vocab)\n","    # load model weights\n","    trained_model.load_state_dict(\n","        torch.load(\"/content/drive/MyDrive/Sign IDD Openpose/Sign-IDD/Checkpoints/final_model.pth\", map_location=device)\n","    )\n","\n","    trainer = TrainManager(model=trained_model, config=cfg, test=True)\n","\n","    # choose the split (dev or test)\n","    for data_set_name, data_set in data_to_predict.items():\n","        print(f\"\\n--- Running inference on {data_set_name.upper()} set ---\")\n","\n","        # validate\n","        valid_score, valid_loss, valid_references, valid_hypotheses, \\\n","        valid_inputs, all_dtw_scores, valid_file_paths, valid_bt_f1, valid_mpjpe, \\\n","        valid_bleu1, valid_bleu4, valid_rouge_l, valid_wer, valid_fid, valid_mpjae = \\\n","            validate_on_data(\n","                model=trained_model,\n","                data=data_set,\n","                batch_size=batch_size,\n","                max_output_length=max_output_length,\n","                eval_metric=eval_metric,\n","                loss_function=None,\n","                batch_type=batch_type,\n","                type=\"val\" if data_set_name != \"train\" else \"train_inf\"\n","            )\n","\n","        # save metrics to text file\n","        result_file = os.path.join(model_dir, f\"{data_set_name}_metrics.txt\")\n","        with open(result_file, 'w') as f:\n","            f.write(f\"\\n===== Evaluation on {data_set_name.upper()} set =====\\n\")\n","            f.write(f\"DTW Score     : {np.mean(all_dtw_scores):.3f}\\n\")\n","            f.write(f\"BT F1 Score   : {valid_bt_f1:.3f}\\n\")\n","            f.write(f\"MPJPE         : {valid_mpjpe:.3f}\\n\")\n","            f.write(f\"BLEU-1 Score  : {valid_bleu1:.3f}\\n\")\n","            f.write(f\"BLEU-4 Score  : {valid_bleu4:.3f}\\n\")\n","            f.write(f\"ROUGE-L Score : {valid_rouge_l:.3f}\\n\")\n","            f.write(f\"WER Score     : {valid_wer:.3f}\\n\")\n","            f.write(f\"FID Score     : {valid_fid:.3f}\\n\")\n","            f.write(f\"MPJAE         : {valid_mpjae:.3f}\\n\")\n","\n","        print(open(result_file).read())\n","\n","        # save predicted skels in the same format as the input skels\n","        display = list(range(len(valid_hypotheses)))\n","        skels_output_path = os.path.join(model_dir, f\"{data_set_name}.pred.skels\")\n","        trainer.save_predicted_skels_txt(\n","            output_joints=valid_hypotheses,\n","            display=display,\n","            output_path=skels_output_path\n","        )\n","\n","        # generate videos\n","        trainer.produce_validation_video(\n","            output_joints=valid_hypotheses,\n","            inputs=valid_inputs,\n","            references=valid_references,\n","            model_dir=model_dir,\n","            display=display,\n","            type=data_set_name,\n","            file_paths=valid_file_paths,\n","            dtw_file=open(result_file, 'a'),\n","        )\n"],"metadata":{"id":"H5nN2uX6i0ha"},"id":"H5nN2uX6i0ha","execution_count":null,"outputs":[]},{"cell_type":"code","source":["test(cfg_file=config_path, ckpt=ckpt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1OQHC3qjzA_TKtbJF-H_Sqhb3qv-9DCzU"},"id":"aRroT9qrqgm-","executionInfo":{"status":"ok","timestamp":1752475698359,"user_tz":-330,"elapsed":417409,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"}},"outputId":"ec6e401b-69f1-454a-cef8-1922f89d732e"},"id":"aRroT9qrqgm-","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# Test with NLP eval metrics"],"metadata":{"id":"jxs75WcG_C1r"},"id":"jxs75WcG_C1r"},{"cell_type":"code","source":["from eval_helper import compute_bleu_scores, compute_rouge, compute_wer\n","import os\n","\n","def evaluate_gloss_file(pred_gloss_path, reference_inputs, reference_file_paths):\n","    # Load predictions as {id: predicted gloss string}\n","    preds = {}\n","    with open(pred_gloss_path, 'r') as f:\n","        for line in f:\n","            sample_id, gloss = line.strip().split('|', 1)\n","            preds[sample_id] = gloss\n","\n","    # Convert reference inputs to strings\n","    refs = [' '.join(tokens) for tokens in reference_inputs]\n","\n","    # Map predictions to correct sample order using file_paths\n","    hyp_texts = []\n","    for path in reference_file_paths:\n","        sample_id = os.path.basename(path).replace('.pkl', '')\n","        hyp_texts.append(preds.get(sample_id, ''))\n","\n","    # Compute metrics\n","    bleu1, bleu4 = compute_bleu_scores(refs, hyp_texts)\n","    rouge_l = compute_rouge(refs, hyp_texts)\n","    wer = compute_wer(refs, hyp_texts)\n","\n","    return bleu1, bleu4, rouge_l, wer"],"metadata":{"id":"y-dKV5xY_Bia","executionInfo":{"status":"ok","timestamp":1752555305498,"user_tz":-330,"elapsed":74,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"}}},"id":"y-dKV5xY_Bia","execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Modified validate_on_data"],"metadata":{"id":"gGDx5U23fbPT"},"id":"gGDx5U23fbPT"},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","import torchtext_compat\n","from data import Dataset, make_data_iterator\n","from torchtext_compat import Batch\n","from helpers import calculate_dtw\n","from eval_helper import calculate_bt_and_mpjpe, compute_rouge, compute_bleu_scores, compute_wer, compute_fid, mpjae\n","from model import Model\n","from constants import PAD_TOKEN, TARGET_PAD\n","\n","\n","# Validate epoch given a dataset\n","def validate_on_data2(model: Model,\n","                     data: Dataset,\n","                     batch_size: int,\n","                     max_output_length: int,\n","                     eval_metric: str,\n","                     loss_function: torch.nn.Module = None,\n","                     batch_type: str = \"sentence\",\n","                     type = \"val\",\n","                     BT_model = None\n","                     ):\n","\n","    valid_iter = make_data_iterator(\n","        dataset=data, fields=data.fields, batch_size=batch_size,\n","        shuffle=True)\n","\n","    pad_index = model.src_vocab.stoi[PAD_TOKEN]\n","    # disable dropout\n","    model.eval()\n","    # don't track gradients during validation\n","    with torch.no_grad():\n","        valid_hypotheses = []\n","        valid_references = []\n","        valid_inputs = []\n","        file_paths = []\n","        all_dtw_scores = []\n","\n","        valid_loss = 0\n","        total_ntokens = 0\n","        total_nseqs = 0\n","\n","        batches = 0\n","        for valid_batch in iter(valid_iter):\n","            # Extract batch\n","            batch = Batch(batch_dict=valid_batch, pad_index=pad_index, target_pad= TARGET_PAD)\n","            targets = batch.trg_input\n","\n","            # run as during training with teacher forcing\n","            if loss_function is not None and batch.trg is not None:\n","                # Get the loss for this batch\n","                batch_loss = model.get_loss_for_batch(is_train=True,\n","                                                         batch=batch,\n","                                                         loss_function=loss_function)\n","\n","                valid_loss += batch_loss\n","                total_ntokens += batch.ntokens\n","                total_nseqs += batch.nseqs\n","\n","            output = model.forward(src=batch.src,\n","                                       trg_input=batch.trg_input[:, :, :150],\n","                                       src_mask=batch.src_mask,\n","                                       src_lengths=batch.src_lengths,\n","                                       trg_mask=batch.trg_mask,\n","                                       is_train=False)\n","\n","            output = torch.cat((output, batch.trg_input[:, :, 150:]), dim=-1)\n","\n","            # Add references, hypotheses and file paths to list\n","            valid_references.extend(targets)\n","            valid_hypotheses.extend(output)\n","            file_paths.extend(batch.file_paths)\n","            # Add the source sentences to list, by using the model source vocab and batch indices\n","            valid_inputs.extend([[model.src_vocab.itos[batch.src[i][j]] for j in range(len(batch.src[i]))] for i in\n","                                 range(len(batch.src))])\n","\n","            # Calculate the full Dynamic Time Warping score - for evaluation\n","            dtw_score = calculate_dtw(targets, output)\n","            all_dtw_scores.extend(dtw_score)\n","\n","            #Calculate bt and mpjpe\n","            bt_f1_scores, mpjpe_scores = calculate_bt_and_mpjpe(targets, output)\n","\n","            # Can set to only run a few batches\n","            # if batches == math.ceil(20/batch_size):\n","            #     break\n","            batches += 1\n","\n","        # Dynamic Time Warping scores\n","        current_valid_score = np.mean(all_dtw_scores)\n","        #Mean bt and mpjpe scores\n","        mean_bt_f1 = np.mean(bt_f1_scores)\n","        mean_mpjpe = np.mean(mpjpe_scores)\n","\n","        #for split_name in [\"dev\", \"test\"]:\n","            #pred_file = f\"/content/drive/MyDrive/Sign IDD Openpose/Sign-IDD/Data/SLT_predicted.{split_name}.gloss\"\n","\n","        pred_file = f\"/content/drive/MyDrive/Sign IDD Openpose/Sign-IDD/Data/SLT_predicted.{type}.gloss\"\n","        bleu1, bleu4, rouge_l, wer = evaluate_gloss_file(\n","                                            pred_file, valid_inputs, file_paths\n","                                           )\n","\n","        # print(f\"\\n=== {type.upper()} Gloss Evaluation ===\")\n","        # print(f\"BLEU-1     : {bleu1:.3f}\")\n","        # print(f\"BLEU-4     : {bleu4:.3f}\")\n","        # print(f\"ROUGE-L    : {rouge_l:.3f}\")\n","        # print(f\"WER        : {wer:.3f}\")\n","\n","\n","        # Convert token lists to text for NLP metrics\n","        #ref_texts = [' '.join(tokens) for tokens in valid_inputs]\n","        #hyp_texts = ref_texts  # Replace this with actual model outputs if you have textual predictions\n","\n","        # NLP metrics\n","        #bleu1, bleu4 = compute_bleu_scores(ref_texts, hyp_texts)\n","        #rouge_l = compute_rouge(ref_texts, hyp_texts)\n","        #print(\"Rouge done\")\n","        #rouge_l=0\n","        #wer_score = compute_wer(ref_texts, hyp_texts)\n","        #print(\"wer done\")\n","\n","        # FID and MPJAE using skeletal predictions\n","        #print(\"Hypotheses:\", len(valid_hypotheses), valid_hypotheses[0].shape)\n","        #print(\"References:\", len(valid_references), valid_references[0].shape)\n","\n","        # Check memory size\n","        #import psutil\n","        #import os\n","        #print(\"RAM usage before FID:\", psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2, \"MB\")\n","\n","\n","        fake_feats = torch.stack(valid_hypotheses).reshape(len(valid_hypotheses), -1).cpu().numpy()\n","        real_feats = torch.stack(valid_references).reshape(len(valid_references), -1).cpu().numpy()\n","        fid_score = compute_fid(fake_feats, real_feats)\n","        #print(\"fid done\")\n","        mpjae_score = mpjae(torch.stack(valid_hypotheses), torch.stack(valid_references))\n","        #print(\"mpjae done\")\n","\n","\n","    return current_valid_score, valid_loss, valid_references, valid_hypotheses, \\\n","       valid_inputs, all_dtw_scores, file_paths, mean_bt_f1, mean_mpjpe, \\\n","       bleu1, bleu4, rouge_l, wer, fid_score, mpjae_score"],"metadata":{"id":"Ksimo769_JzP","executionInfo":{"status":"ok","timestamp":1752555562697,"user_tz":-330,"elapsed":134,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"}}},"id":"Ksimo769_JzP","execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### Final test function"],"metadata":{"id":"TcLDylSkfl9E"},"id":"TcLDylSkfl9E"},{"cell_type":"code","source":["def test2(cfg_file, ckpt: str):\n","    cfg = load_config(cfg_file)\n","    model_dir = cfg[\"training\"][\"model_dir\"]\n","\n","    batch_size = cfg[\"training\"].get(\"eval_batch_size\", cfg[\"training\"][\"batch_size\"])\n","    batch_type = cfg[\"training\"].get(\"eval_batch_type\", cfg[\"training\"].get(\"batch_type\", \"sentence\"))\n","    use_cuda = cfg[\"training\"].get(\"use_cuda\", True)\n","    eval_metric = cfg[\"training\"][\"eval_metric\"]\n","    max_output_length = cfg[\"training\"].get(\"max_output_length\", None)\n","\n","    _, dev_data, test_data, src_vocab, trg_vocab = load_data(cfg=cfg)\n","    data_to_predict = {\"dev\": dev_data, \"test\": test_data}\n","\n","    trained_model = build_model(cfg, src_vocab, trg_vocab)\n","    trained_model.load_state_dict(\n","        torch.load(\"/content/drive/MyDrive/Sign IDD Openpose/Sign-IDD/Checkpoints/final_model.pth\", map_location=device)\n","    )\n","\n","    trainer = TrainManager(model=trained_model, config=cfg, test=True)\n","\n","    for data_set_name, data_set in data_to_predict.items():\n","        print(f\"\\n--- Running inference on {data_set_name.upper()} set ---\")\n","\n","        valid_score, valid_loss, valid_references, valid_hypotheses, \\\n","        valid_inputs, all_dtw_scores, valid_file_paths, valid_bt_f1, valid_mpjpe, \\\n","        valid_bleu1, valid_bleu4, valid_rouge_l, valid_wer, valid_fid, valid_mpjae = \\\n","            validate_on_data2(\n","                model=trained_model,\n","                data=data_set,\n","                batch_size=batch_size,\n","                max_output_length=max_output_length,\n","                eval_metric=eval_metric,\n","                loss_function=None,\n","                batch_type=batch_type,\n","                type=data_set_name,\n","            )\n","\n","        # Save metrics to file\n","        result_file = os.path.join(model_dir, f\"{data_set_name}_metrics.txt\")\n","        with open(result_file, 'w') as f:\n","            f.write(f\"\\n===== Evaluation on {data_set_name.upper()} set =====\\n\")\n","            f.write(f\"DTW Score     : {np.mean(all_dtw_scores):.3f}\\n\")\n","            f.write(f\"BT F1 Score   : {valid_bt_f1:.3f}\\n\")\n","            f.write(f\"MPJPE         : {valid_mpjpe:.3f}\\n\")\n","            f.write(f\"BLEU-1 Score  : {valid_bleu1:.3f}\\n\")\n","            f.write(f\"BLEU-4 Score  : {valid_bleu4:.3f}\\n\")\n","            f.write(f\"ROUGE-L Score : {valid_rouge_l:.3f}\\n\")\n","            f.write(f\"WER Score     : {valid_wer:.3f}\\n\")\n","            f.write(f\"FID Score     : {valid_fid:.3f}\\n\")\n","            f.write(f\"MPJAE         : {valid_mpjae:.3f}\\n\")\n","\n","        print(open(result_file).read())\n","\n","        # Save predicted skels in correct format\n","        display = list(range(len(valid_hypotheses)))\n","        skels_output_path = os.path.join(model_dir, f\"{data_set_name}.pred.skels\")\n","        trainer.save_predicted_skels_txt(\n","            output_joints=valid_hypotheses,\n","            display=display,\n","            output_path=skels_output_path\n","        )\n","\n","        # Optional: Generate videos\n","        trainer.produce_validation_video(\n","            output_joints=valid_hypotheses,\n","            inputs=valid_inputs,\n","            references=valid_references,\n","            model_dir=model_dir,\n","            display=display,\n","            type=data_set_name,\n","            file_paths=valid_file_paths,\n","            dtw_file=open(result_file, 'a'),\n","        )\n"],"metadata":{"id":"TX_bstwyBwIH","executionInfo":{"status":"ok","timestamp":1752555313220,"user_tz":-330,"elapsed":47,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"}}},"id":"TX_bstwyBwIH","execution_count":12,"outputs":[]},{"cell_type":"code","source":["test2(cfg_file=config_path,ckpt=ckpt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Jq6L3EQ0H5eo3n3r1JLNsLfs1IACJgxa"},"id":"4VnzQHhpB-gq","executionInfo":{"status":"ok","timestamp":1752555865628,"user_tz":-330,"elapsed":297321,"user":{"displayName":"Hamsini G","userId":"15879291242945085196"}},"outputId":"855ea500-6914-4660-8c66-7954670db767"},"id":"4VnzQHhpB-gq","execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","id":"f7aaf284","metadata":{"id":"f7aaf284"},"source":["# **DATA FLOW**\n","\n","## Dataset preparation (load_data)\n","- glosses are tokenized, src_vocab is created (.src)\n","- skels are converted into a list of N(number of frames) 1d tensors of size 151 (150 keypoints + 1 frame counter)-> N frames with 150 keypoints each (.trg)\n","- files (mp4 paths)- no processing (.file_paths)\n","- each element in train_data is a triplet (.src, .trg, .file_path)\n","\n","## Preprocessing and batching (make_data_iter)\n","- src - convert gloss tokens into indices (tensors) using vocab, group tensors based on batch size\n","- trg - each element has a list of 2d tensors (frames, [151]), pads all sequences in the batch acc to the sequence with max no of frames and returns a 3d tensor (batch size, frames, [151])\n","- file_paths - no changes\n","- each batch object is a dictionary of the above 3 preprocessed fields\n","\n","## Training\n","- glosses - embedded and passed through transformer encoder layer\n","- target poses(skels)\n","      - passed through acd\n","      - id is applied for better structural representation of the poses  (3d joints are converted into features- 7d vector per bone (3d coordinates+bonelength+3d unit direction)\n","      - diffusion (noise is added to ground truth poses)\n","- acd denoiser uses encoded glosses to reverse noise in the ground truth poses and produces pred_poses\n","- loss is calculated by comparing the original ground truth poses (without noise) and pred_poses\n","\n","## Validation\n","- dtw scores are calculated using ground truth and predicted poses (unseen data)\n","- videos are rendered for best predictions (lower dtw)\n","- b1, b4, rouge and wer metrics are calculated using the predicted glosses produced by the backtranslator Sign IDD SLT (converts the predicted poses of Sign IDD back to gloss sequences)\n","- additional metrics include fid, mpjpe, mpjae and bt f1\n"]},{"cell_type":"markdown","id":"ifdYJEeVVM2y","metadata":{"id":"ifdYJEeVVM2y"},"source":["# Model architecture visualisation"]},{"cell_type":"code","execution_count":null,"id":"127d357e","metadata":{"id":"127d357e"},"outputs":[],"source":["!{sys.executable} -m pip install torchview\n"]},{"cell_type":"code","execution_count":null,"id":"aa10d810","metadata":{"id":"aa10d810"},"outputs":[],"source":["import torch.nn as nn\n","class WrappedModel(nn.Module):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.model = model\n","\n","    def forward(self, inputs):\n","        return self.model(\n","            is_train=False,\n","            src=inputs[0],\n","            trg_input=inputs[1],\n","            src_mask=inputs[2],\n","            src_lengths=inputs[3],\n","            trg_mask=inputs[4]\n","        )\n"]},{"cell_type":"code","execution_count":null,"id":"b09927f0","metadata":{"id":"b09927f0","scrolled":true},"outputs":[],"source":["from torchview import draw_graph\n","#from wrapped_model import WrappedModel  # make sure this class is defined\n","\n","cfg= load_config(config_path)\n","_, _, _, src_vocab, trg_vocab = load_data(cfg=cfg)\n","model = build_model(cfg, src_vocab, trg_vocab).to(device)\n","wrapped_model = WrappedModel(model)\n","\n","# Extract sizes from config or model\n","vocab_size = len(src_vocab)\n","src_len = 20               # example input sentence length (you can choose 20–40)\n","pose_len = 30              # number of frames (temporal dimension)\n","pose_dim = cfg[\"model\"][\"trg_size\"]  # e.g., 151\n","\n","# Create dummy inputs\n","dummy_src = torch.randint(0, vocab_size, (1, src_len)).to(device)\n","dummy_trg = torch.randn(1, pose_len, pose_dim).to(device)\n","dummy_src_mask = torch.ones(1, 1, src_len).bool().to(device)\n","dummy_src_lengths = torch.tensor([src_len]).to(device)\n","dummy_trg_mask = torch.ones(1, 1, pose_len).bool().to(device)\n","\n","input_tuple = (dummy_src, dummy_trg, dummy_src_mask, dummy_src_lengths, dummy_trg_mask)\n","\n","# Visualize the model\n","graph = draw_graph(\n","    wrapped_model,\n","    input_data=(input_tuple,),\n","    input_size=None,\n","    expand_nested=True,\n","    save_graph=True,\n","    directory=\".\",\n","    filename=\"model_architecture\",\n","    roll=True\n",")\n","\n","graph.visual_graph  # displays inside Jupyter\n"]},{"cell_type":"markdown","id":"849bfbce","metadata":{"id":"849bfbce"},"source":["### 151st value??? counter"]},{"cell_type":"code","execution_count":null,"id":"6d7d1fa5","metadata":{"id":"6d7d1fa5"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Path to your file\n","file_path = \"./Data/train.skels\"\n","\n","# Read and parse the file\n","with open(file_path, 'r') as f:\n","    lines = f.readlines()\n","\n","# Assume 151 values per frame\n","frame_dim = 151\n","feature_dim = 150\n","\n","# Parse and visualize each line\n","for i, line in enumerate(lines):\n","    values = np.fromstring(line.strip(), sep=' ')\n","    num_frames = len(values) // frame_dim\n","    data = values.reshape(num_frames, frame_dim)\n","\n","    features = data[:, :feature_dim]\n","    counters = data[:, -1]\n","\n","\n","    print(f\"\\nLine {i+1} - {num_frames} frames\")\n","    print(\"Frame\\tCounter\\tSample Features\")\n","    j=0;\n","    for f in range(num_frames):\n","        print(f\"{f+1}\\t{counters[f]}\\t{features[f, :5]} ...\")\n","\n","    # Optional: Plot counter values across frames\n","    plt.figure(figsize=(8, 2))\n","    plt.plot(counters, marker='o')\n","    plt.title(f\"Line {i+1} - Frame Counters\")\n","    plt.xlabel(\"Frame\")\n","    plt.ylabel(\"Counter\")\n","    plt.grid(True)\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"fc97f641","metadata":{"id":"fc97f641"},"outputs":[],"source":["# Load the file and read all numerical values into a list\n","file_path = \"./Data/train.skels\"\n","\n","# Read the values from the file\n","with open(file_path, \"r\") as f:\n","    data = f.read()\n","\n","# Convert the data into a list of floats\n","values = [float(x) for x in data.strip().split()]\n","\n","# Extract every 151st value (multiples of 151, 1-based index)\n","multiples_of_151 = [values[i - 1] for i in range(151, len(values) + 1, 151)]\n","\n","multiples_of_151[:86]  # Show first 86 such values as a sample\n"]},{"cell_type":"code","source":[],"metadata":{"id":"fWX6BqnxItoz"},"id":"fWX6BqnxItoz","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["6caf255c-18d5-43dd-bb54-a097d255f172","6d0371d1","b0931489","gGDx5U23fbPT","ifdYJEeVVM2y"]},"kernelspec":{"display_name":"Python (cv2env)","language":"python","name":"cv2env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}